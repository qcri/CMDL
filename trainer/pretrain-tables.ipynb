{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e082e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## configurations\n",
    "from types import SimpleNamespace   \n",
    "\n",
    "config = {}\n",
    "config['train_frac'] = 1 # Increase to generate and train with more triplets. Can improve performance\n",
    "config['epochs'] = 1\n",
    "config['batch_size'] = 16\n",
    "config['final_size'] = 200\n",
    "config['lr'] = .00001\n",
    "config['loss'] = 'tripletCE'\n",
    "config['tl_margin'] = 1.0\n",
    "config['tl_p'] = 2\n",
    "config['pool_type'] = \"CLS\"\n",
    "config['tokenizer_max_length'] = 512\n",
    "config['temp'] = 0.05\n",
    "config['init_model_path'] = './roberta-retrained'\n",
    "config['input_data_dir'] = '../inputs/mlopen-tables'\n",
    "config['separator'] = ','\n",
    "config['max_header_encoding_length'] = 27\n",
    "config['max_tuple_content_encoding_length'] = 482\n",
    "config['max_tuples_per_table'] = 500\n",
    "config['max_data_length'] = 50000\n",
    "config['train_size'] = 40000\n",
    "config['test_size'] = 10000\n",
    "\n",
    "conf = SimpleNamespace(**config)\n",
    "\n",
    "## TODO: Save config to disk and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5942aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## table data read and encode\n",
    "import os, sys, random\n",
    "# add main directory in sys path to access utility functions\n",
    "path2add = os.path.normpath(os.path.abspath('..'))\n",
    "if (not (path2add in sys.path)) :\n",
    "    sys.path.append(path2add)\n",
    "\n",
    "from profiler.table_profiler_lm import table_profiler_lm\n",
    "from table_trainer_utils import *\n",
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "## create two datasets, one with original sequence, and its augmentation with transformations, also return list of row ids\n",
    "def encode_tuples(profiler, encoder, augmentor, data_dir):\n",
    "    cnt = 0\n",
    "    tupleids = []\n",
    "    tokens = []\n",
    "    aug_tokens = []\n",
    "    \n",
    "    for id, tblname_list, colname_list, _, content_list, _ in profiler.process_dir(data_dir, sep=','):\n",
    "        cnt += 1\n",
    "        tupleids.append(id)\n",
    "\n",
    "        full_row_enc = encoder.encode_row(tblname_list, colname_list, content_list)\n",
    "        tokens.append(full_row_enc)\n",
    "        \n",
    "        tbl, col, cont = augmentor.augment(tblname_list, colname_list, content_list)\n",
    "        aug_row_enc = encoder.encode_row(tbl, col, cont)\n",
    "        aug_tokens.append(aug_row_enc)\n",
    "        \n",
    "    print('Encoded ', cnt, ' rows!')\n",
    "        \n",
    "    return tokens, aug_tokens, tupleids\n",
    "                 \n",
    "data_dir = conf.input_data_dir\n",
    "profiler = table_profiler_lm(tokenizer=tokenizer, \n",
    "                             max_tuples_per_table=conf.max_tuples_per_table)\n",
    "encoder = TupleEncoder(tokenizer, conf)\n",
    "augmentor = TupleAugmentor()\n",
    "orig, aug, tupleids = encode_tuples(profiler, encoder, augmentor, data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8322f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model checkpoint pre-trained on corresponding text corpus\n",
    "from transformers import RobertaTokenizer\n",
    "model=conf.init_model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf2776f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset of length:  50000\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = conf.max_data_length\n",
    "tuple_dataset = TupleDataset(tokenizer, \n",
    "                             tupleids[:MAX_LEN], \n",
    "                             orig[:MAX_LEN], \n",
    "                             aug[:MAX_LEN])\n",
    "\n",
    "print('Created dataset of length: ', len(tuple_dataset))\n",
    "\n",
    "# data_loader = TupleDataloader(tuple_dataset, conf)\n",
    "# print(dir(data_loader))\n",
    "# training_data, test_data = data_loader.training_dataloader(), data_loader.test_dataloader()\n",
    "training_data, test_data = torch.utils.data.random_split(tuple_dataset, \n",
    "                                                          [conf.train_size, conf.test_size],\n",
    "                                                          torch.Generator().manual_seed(42))\n",
    "data_loader = torch.utils.data.DataLoader(training_data, batch_size=conf.batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f9550dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train models for tuples\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "def save_torch_model(path, model):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    filepath = os.path.join(path, datetime.now().strftime(\"%H-%M-%d-%m-%y\"))\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    print(f\"Saved Model: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "class Similarity(nn.Module):\n",
    "    \"\"\"\n",
    "    Dot product or cosine similarity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, temp):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "        self.cos = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        similarities = self.cos(x, y)\n",
    "        return similarities / self.temp\n",
    "\n",
    "def crossentropy_loss(a_pred, p_pred, n_pred, device=\"cuda\"):\n",
    "    sim_fn = Similarity(0.05) # FIXME: hardcoded temp\n",
    "    idxs = torch.arange(0, a_pred.shape[0], device=device)\n",
    "    y_true = idxs#idxs + 1 - idxs % 2 * 2 # each example is paired with its p counterpart\n",
    "    similarities = sim_fn(a_pred.unsqueeze(1), p_pred.unsqueeze(0))\n",
    "    loss = F.cross_entropy(similarities, y_true)\n",
    "    return torch.mean(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73b5aeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing iter 4100 with loss=0.001954467035830021: : 4101it [2:48:48,  2.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss: 0.001954467035830021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing iter 4200 with loss=0.0031837301794439554: : 4201it [2:52:53,  2.38s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss: 0.0031837301794439554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing iter 4374 with loss=0.0005818585050292313: : 4375it [3:00:03,  2.47s/it] \n"
     ]
    }
   ],
   "source": [
    "## training\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_table_model(model, \n",
    "                    train_data, \n",
    "                    loss_func, \n",
    "                    optimizer, \n",
    "                    epochs, \n",
    "                    save_dir,\n",
    "                    tokenizer_max_length = 512,\n",
    "                    train_model = True) -> str:\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        pbar = tqdm(enumerate(train_data))\n",
    "        for i, d in pbar:\n",
    "#             print('Working on index: ', i, ' data: ', d['pivot_ids'].shape)\n",
    "            a, p, n = d['pivot_ids'], d['positive_ids'], d['negative_ids']\n",
    "            a_mask, p_mask, n_mask = d['pivot_attn'], d['positive_attn'], d['negative_attn']\n",
    "            for d in [a, p, n, a_mask, p_mask, n_mask]:\n",
    "                d.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            oa, op, on = model(a, p, n, a_mask, p_mask, n_mask)\n",
    "            loss = loss_func(oa, op, on)\n",
    "            pbar.set_description(f\"Processing iter {i} with loss={loss.item()}\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i % 100) == 0 : \n",
    "                    tqdm.write(f\"train batch loss: {loss.item()}\")\n",
    "            if (i % 500 == 0):\n",
    "                    save_torch_model(save_dir, model)\n",
    "\n",
    "        \n",
    "if conf.loss == 'triplet':\n",
    "    loss = nn.TripletMarginLoss(margin=conf.tl_margin, p=conf.tl_p)\n",
    "elif conf.loss == 'tripletCE':\n",
    "    loss = crossentropy_loss\n",
    "model = TripletSingleBERTModel(final_size = conf.final_size, \n",
    "                                   tokenizer = tokenizer, \n",
    "                                   pooling = conf.pool_type, \n",
    "                                   model_path = conf.init_model_path)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=conf.lr)\n",
    "\n",
    "# save_dir = param_header(conf.batch_size, conf.final_size, conf.lr, conf.pool_type, conf.epochs, conf.train_size)\n",
    "save_dir = './mlopen-table-model'#f'{conf.data_path}/models/emb/{conf.model_name}/{save_dir}/'\n",
    "\n",
    "# training_data = data_loader.training_data() #DataLoader(tuple_dataset, batch_size=conf.batch_size, shuffle=True)\n",
    "tqdm.write(\"Training Begins\")\n",
    "last_saved = train_table_model(model, \n",
    "                               data_loader, \n",
    "                               loss, \n",
    "                               optimizer, \n",
    "                               conf.epochs, \n",
    "                               save_dir, \n",
    "                               train_model=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc4e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluate\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaModel\n",
    "from scipy.spatial.distance import cosine\n",
    "from table_trainer_utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def read_pretrained_model(model_path):\n",
    "    return RobertaModel.from_pretrained(model_path)\n",
    "\n",
    "def read_model_from_statedict(model_path):\n",
    "    model = TripletSingleBERTModel(final_size = conf.final_size, \n",
    "                                   tokenizer = tokenizer, \n",
    "                                   pooling = conf.pool_type, \n",
    "                                   model_path=conf.init_model_path)    \n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "model2 = read_model_from_statedict('./mlopen-table-model/14-45-08-11-22') ##Hardcoded\n",
    "model2.to(device)\n",
    "model2.eval()\n",
    "print('Loaded model:', model2)\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "# eval_data = data_loader.test_data()#DataLoader(eval_dataset, batch_size=5, shuffle=False)\n",
    "eval_data = torch.utils.data.DataLoader(test_data, batch_size=conf.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "pbar = tqdm(enumerate(eval_data))\n",
    "for i, d in pbar:\n",
    "    print('Working on index: ', i, ' data: ', d['pivot_ids'].shape)\n",
    "    a, p, n = d['pivot_ids'], d['positive_ids'], d['negative_ids']\n",
    "    a_mask, p_mask, n_mask = d['pivot_attn'], d['positive_attn'], d['negative_attn']\n",
    "    embdA, embdP, embdN = model2(a, p, n, a_mask, p_mask, n_mask)\n",
    "    print(f'embed shape: {embdA.shape}')\n",
    "    print(f'first cos: {cos(embdA, embdP)}')\n",
    "    print(f'second cos: {cos(embdA, embdN)}')\n",
    "    break                      \n",
    "\n",
    "    \n",
    "# loss function is motivated from simcse work: https://github.com/princeton-nlp/SimCSE/blob/main/simcse/models.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d78fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save embeddings to disk\n",
    "import csv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def write_csv(fp, rows):\n",
    "  with open(fp, 'w') as f:\n",
    "    csvf = csv.writer(f)\n",
    "    [csvf.writerow([r]) for r in rows]\n",
    "\n",
    "output_ids_path = '../features/mlopen-tupleids.list'\n",
    "output_embed_path = '../features/mlopen-tuplefeatures.pt'\n",
    "\n",
    "tuple_ids = [tuple_dataset.get_tuple_id(i) for i in range(0, len(tuple_dataset))]\n",
    "data_loader = torch.utils.data.DataLoader(tuple_dataset, \n",
    "                                          batch_size=conf.batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "write_csv(output_ids_path, tuple_ids)\n",
    "\n",
    "embed_tensor = torch.empty((len(tuple_dataset), conf.final_size))\n",
    "print('shape of empty: ', embed_tensor.shape)\n",
    "for i, d in tqdm(enumerate(data_loader)):\n",
    "    a, p, n = d['pivot_ids'], d['positive_ids'], d['negative_ids']\n",
    "    a_mask, p_mask, n_mask = d['pivot_attn'], d['positive_attn'], d['negative_attn']\n",
    "    embdA, embdP, embdN = model2(a, p, n, a_mask, p_mask, n_mask)\n",
    "    op = embdA.detach().cpu()\n",
    "    embed_tensor[i*conf.batch_size: (i+1)*conf.batch_size] = op\n",
    "    \n",
    "print('shape of output: ', embed_tensor.shape, ' output: ', embed_tensor)\n",
    "torch.save(embed_tensor, output_embed_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
